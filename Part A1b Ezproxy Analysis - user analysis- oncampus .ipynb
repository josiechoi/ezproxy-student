{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note: \n",
    "This is the exact set of code in Part A1a. But this one will mark the file as oncampus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE WORKS AS WELL. MODIFED FROM CODE BY Petrina Collingwood\n",
    "\n",
    "\n",
    "def domain_count(filename):\n",
    "    \n",
    "    import csv\n",
    "    import re\n",
    "    # create csv file from log file\n",
    "    with open(filename,'r') as fh:\n",
    "        with open('csv/' + filename + '.csv','w') as outfile:\n",
    "            for line in fh:\n",
    "                print(re.sub(r'\\n|\"','',line), file=outfile)\n",
    "    import pandas as pd\n",
    "    from urllib.parse import unquote\n",
    "    # create dataframe from csv file skipping malformed lines\n",
    "    df = pd.read_csv('csv/' + filename + '.csv',sep=' ', error_bad_lines=False, header=None, encoding='utf-8')\n",
    "    # remove unnecessary columns\n",
    "    df.drop(df.columns[[2,5,6,8,9]], axis=1, inplace=True)\n",
    "    # name columns\n",
    "    df.columns = ['ip', 'session_id', 'user_id', 'date_time', 'url', 'size']\n",
    "    # formate date/time column\n",
    "    df['date_time'] = df['date_time'].map(lambda x: x.lstrip('['))\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], format='%d/%b/%Y:%H:%M:%S')\n",
    "    # remove lines where user is not logged in\n",
    "    df = df[df.user_id != \"-\"]\n",
    "    # decode urls\n",
    "    def decode_url(url):\n",
    "        decoded_url = unquote(url)\n",
    "        return decoded_url\n",
    "    df['url'] = df.url.apply(decode_url)\n",
    "    # remove excess columns for domain\n",
    "    df.drop(['ip','session_id','size'], axis=1, inplace=True)\n",
    "    # remove ezp string from start of url\n",
    "    df['url'] = df['url'].str.replace(r'^http://ezproxy\\.lib\\.ryerson\\.ca/login/\\?url=', '')# remove http etc\n",
    "    df['url'] = df['url'].str.replace(r'^http://www\\.|^https://www\\.|^http://|^https://', '')\n",
    "    # remove ezproxy string from start of url\n",
    "    def parse_url(url):\n",
    "        if (url.startswith(\"ezproxy.lib.ryerson.ca/login?url=\")) and (\"http\" in url):\n",
    "            location = url.find(\"http\")\n",
    "            return url[location:]\n",
    "        elif (url.startswith(\"ezproxy.lib.ryerson.ca/login?url=\")):\n",
    "            return \"-\"\n",
    "        else:\n",
    "            return url\n",
    "    df['url'] = df.url.apply(parse_url)\n",
    "    # remove http etc\n",
    "    df['url'] = df['url'].str.replace(r'^http://www\\.|^https://www\\.|^http://|^https://', '')\n",
    "    # remove rows where ezproxy string is the only url\n",
    "    df = df[df.url != \"-\"]\n",
    "    # remove spaces introduced by unquoting\n",
    "    df['url'] = df['url'].str.replace(r'\\n', '')\n",
    "    # remove everything after : or / or ?\n",
    "    df['url'] = df['url'].str.replace(r'[:/?].*$', '')\n",
    "    # remove .ezp.lib.unimelb.edu.au from urls\n",
    "    df['url'] = df['url'].str.replace(r'ezproxy\\.lib\\.ryerson\\.ca', '')\n",
    "    df['url'] = df['url'].str.replace(r'ezproxy\\.lib\\.', '-')\n",
    "    df = df[df.url != \"-\"]\n",
    "    # create new column of domains\n",
    "    def get_domain(url):\n",
    "        regexp = re.compile(r'\\.com|\\.org|\\.net|\\.edu|-org|-com|\\.gov')\n",
    "        if regexp.search(url) is not None:\n",
    "            for match in regexp.finditer(url):\n",
    "                location = match.start()\n",
    "            new_url = url[:location]\n",
    "            if ('.' in new_url):\n",
    "                location = new_url.rfind('.')\n",
    "            elif ('-' in new_url):\n",
    "                location = new_url.rfind('-')\n",
    "            else:\n",
    "                return url\n",
    "            location += 1\n",
    "            \n",
    "            if (\"-org\" in url[location:]):\n",
    "                modified_url = url[location:].replace(r'-org', '.org')\n",
    "            elif (\"-com\" in url[location:]):\n",
    "                modified_url = url[location:].replace(r'-com', '.com')\n",
    "            else:\n",
    "                return url[location:]\n",
    "            return modified_url\n",
    "        else:\n",
    "            return url\n",
    "    df['domain'] = df.url.apply(get_domain)\n",
    "    \n",
    "    \n",
    "    # remove duplicate rows which have same user_id, date-time and domain. \n",
    "    df.drop_duplicates(subset=['date_time','domain'], inplace=True)\n",
    "\n",
    "    df.to_csv('student_count.csv',index=False, encoding='utf-8')\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A folder needs to be created in csv in order for this to work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE WORKS !!!\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "studentdf = pd.read_csv(\"data/student1718.csv\")\n",
    "newdf=pd.DataFrame()\n",
    "\n",
    "for filename in glob.iglob(os.path.join(\"oncampus/201808/\",\"*.log\")):\n",
    "    filename = filename.replace(\"\\\\\",\"/\")\n",
    "    # call log analysis function \n",
    "    domain_count(filename)\n",
    "    # create dataframe from csv output file from daily domain count\n",
    "    df=pd.read_csv(\"student_count.csv\",sep=\",\")\n",
    "    df_combined=pd.merge(df,studentdf,how=\"left\",left_on=\"user_id\",right_on=\"USERNAME\")\n",
    "    df_combined.columns=['user_id','date_time','url','domain','USERNAME',\"acad_prog\",\"prog_yr\"]\n",
    "    df_combined=df_combined.drop('USERNAME',axis=1)\n",
    "    df_combined['acad_prog']=df_combined['acad_prog'].fillna(\"staff\")\n",
    "    df_combined['prog_yr']=df_combined['prog_yr'].fillna(\"Other\")\n",
    "    df_combined['on_or_off']=\"oncampus\"\n",
    "    newdf = newdf.append(df_combined)  \n",
    "    newdf.to_csv('combined201808_oncampus.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined201712_oncampus.csv\",sep=\",\")\n",
    "dfg=df.groupby([\"domain\",\"acad_prog\"])['prog_yr'].count().unstack().fillna(0)\n",
    "dfp=df.groupby([\"domain\",\"prog_yr\"])['prog_yr'].count().unstack().fillna(0)\n",
    "dfg.head()\n",
    "dfp.head()\n",
    "\n",
    "#the name of the file needs to be updated for each monthly file\n",
    "\n",
    "dfg.to_excel(\"grouped_data_acadprog201712_oncampus.xls\")\n",
    "dfp.to_excel(\"group_data_progyr_201712oncampus.xls\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

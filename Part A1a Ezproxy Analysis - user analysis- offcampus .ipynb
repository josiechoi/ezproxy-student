{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY Conference 2019 \n",
    "## Usage Data Analysis Using Python \n",
    "### Presenters: Lei Jin, Josephine Choi \n",
    "\n",
    "### https://trylibraryconference.wordpress.com/presentation-descriptions-3/#session3pres6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Synposis \n",
    "COUNTER data provided by publishers quantifies use by title for journals, databases, and ebooks. However, we are unable to see how widely spread or concentrated this use is amongst our University population, and lack details to inform our qualitative information gathering for interdisciplinary content. With the adoption of open source and Python technologies, this ongoing project aims to provide comprehensive in-depth e-resources usage reports by combining EZProxy usage data and student registration data. After final data integration and report generation, we are able to see the use of electronic resources by departments, by student profiles (year, program), thus provide collection development support to subject liaisons. This additional context is useful for decision making, marketing, and communication. It helps to answer questions concerning collection development, such as, what are the top 10 most popular resources used by one certain department? Which student group would be most affected with the cancellation of one certain resource? Or how does a certain department use the resources they requested? etc. The open source and Python scripts are easily shared for those who are interested and desire to adopt this methodology in their own settings.\n",
    "\n",
    "Lei Jin Electronic Resources Librarian Collection, Ryerson University Library and Archives\n",
    "Josephine Choi Library Technician â€“ ER, Acquisition, and Serials Collection, Ryerson University Library and Archives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Acknowledgement \n",
    "\n",
    "The code in this notebook is based on code authored by Petrina Collingwood to get \"monthly/year domain hits/usage from Ezproxy logs\". Collingwood's code counts usage stats per second, and thus \"eliminates counting of excess lines in the log that were generated by one click and therefore makes the data more realistic\" \n",
    "\n",
    "More about Collingwood's project can be found in her Github(https://github.com/prcollingwood/ezproxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using this notebook \n",
    "- We used Anaconda (Python 3.5) for this project \n",
    "- Modification based on ezproxy log file format will be required \n",
    "- Edit of ezproxy prefix will be required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFED FROM CODE BY Petrina Collingwood  \n",
    "\n",
    "\n",
    "def domain_count(filename):\n",
    "    \n",
    "    import csv\n",
    "    import re\n",
    "    # create csv file from log file\n",
    "    with open(filename,'r') as fh:\n",
    "        with open('csv/' + filename + '.csv','w') as outfile:\n",
    "            for line in fh:\n",
    "                print(re.sub(r'\\n|\"','',line), file=outfile)\n",
    "    import pandas as pd\n",
    "    from urllib.parse import unquote\n",
    "    # create dataframe from csv file skipping malformed lines\n",
    "    df = pd.read_csv('csv/' + filename + '.csv',sep=' ', error_bad_lines=False, header=None, encoding='utf-8')\n",
    "    # remove unnecessary columns\n",
    "    df.drop(df.columns[[2,5,6,8,9]], axis=1, inplace=True)\n",
    "    # name columns\n",
    "    df.columns = ['ip', 'session_id', 'user_id', 'date_time', 'url', 'size']\n",
    "    # formate date/time column\n",
    "    df['date_time'] = df['date_time'].map(lambda x: x.lstrip('['))\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], format='%d/%b/%Y:%H:%M:%S')\n",
    "    # remove lines where user is not logged in\n",
    "    df = df[df.user_id != \"-\"]\n",
    "    # decode urls\n",
    "    def decode_url(url):\n",
    "        decoded_url = unquote(url)\n",
    "        return decoded_url\n",
    "    df['url'] = df.url.apply(decode_url)\n",
    "    # remove excess columns for domain\n",
    "    df.drop(['ip','session_id','size'], axis=1, inplace=True)\n",
    "    # remove ezp string from start of url\n",
    "    df['url'] = df['url'].str.replace(r'^http://ezproxy\\.lib\\.ryerson\\.ca/login/\\?url=', '')# remove http etc\n",
    "    df['url'] = df['url'].str.replace(r'^http://www\\.|^https://www\\.|^http://|^https://', '')\n",
    "    # remove ezproxy string from start of url\n",
    "    def parse_url(url):\n",
    "        if (url.startswith(\"ezproxy.lib.ryerson.ca/login?url=\")) and (\"http\" in url):\n",
    "            location = url.find(\"http\")\n",
    "            return url[location:]\n",
    "        elif (url.startswith(\"ezproxy.lib.ryerson.ca/login?url=\")):\n",
    "            return \"-\"\n",
    "        else:\n",
    "            return url\n",
    "    df['url'] = df.url.apply(parse_url)\n",
    "    # remove http etc\n",
    "    df['url'] = df['url'].str.replace(r'^http://www\\.|^https://www\\.|^http://|^https://', '')\n",
    "    # remove rows where ezproxy string is the only url\n",
    "    df = df[df.url != \"-\"]\n",
    "    # remove spaces introduced by unquoting\n",
    "    df['url'] = df['url'].str.replace(r'\\n', '')\n",
    "    # remove everything after : or / or ?\n",
    "    df['url'] = df['url'].str.replace(r'[:/?].*$', '')\n",
    "    # remove .ezp.lib.unimelb.edu.au from urls\n",
    "    df['url'] = df['url'].str.replace(r'ezproxy\\.lib\\.ryerson\\.ca', '')\n",
    "    df['url'] = df['url'].str.replace(r'ezproxy\\.lib\\.', '-')\n",
    "    df = df[df.url != \"-\"]\n",
    "    # create new column of domains\n",
    "    def get_domain(url):\n",
    "        regexp = re.compile(r'\\.com|\\.org|\\.net|\\.edu|-org|-com|\\.gov')\n",
    "        if regexp.search(url) is not None:\n",
    "            for match in regexp.finditer(url):\n",
    "                location = match.start()\n",
    "            new_url = url[:location]\n",
    "            if ('.' in new_url):\n",
    "                location = new_url.rfind('.')\n",
    "            elif ('-' in new_url):\n",
    "                location = new_url.rfind('-')\n",
    "            else:\n",
    "                return url\n",
    "            location += 1\n",
    "            \n",
    "            if (\"-org\" in url[location:]):\n",
    "                modified_url = url[location:].replace(r'-org', '.org')\n",
    "            elif (\"-com\" in url[location:]):\n",
    "                modified_url = url[location:].replace(r'-com', '.com')\n",
    "            else:\n",
    "                return url[location:]\n",
    "            return modified_url\n",
    "        else:\n",
    "            return url\n",
    "    df['domain'] = df.url.apply(get_domain)\n",
    "    \n",
    "    \n",
    "    # remove duplicate rows which have same user_id, date-time and domain. \n",
    "    df.drop_duplicates(subset=['date_time','domain'], inplace=True)\n",
    "   \n",
    "    df.to_csv('student_count.csv',index=False, encoding='utf-8')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A folder needs to be created in csv in order for this to work \n",
    "\n",
    "This part of the code is to combined the simplified ezproxy file with student data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1998: expected 11 fields, saw 19\\nSkipping line 1999: expected 11 fields, saw 18\\nSkipping line 2000: expected 11 fields, saw 18\\nSkipping line 2001: expected 11 fields, saw 18\\nSkipping line 2002: expected 11 fields, saw 17\\nSkipping line 2003: expected 11 fields, saw 17\\n'\n"
     ]
    }
   ],
   "source": [
    "#THIS CODE WORKS !!!\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "studentdf = pd.read_csv(\"data/student1819.csv\")\n",
    "newdf=pd.DataFrame()\n",
    "\n",
    "# this needs to be changed for each month \n",
    "\n",
    "for filename in glob.iglob(os.path.join(\"offcampus/201808/\",\"*.log\")):\n",
    "    filename = filename.replace(\"\\\\\",\"/\")\n",
    "    # call log analysis function \n",
    "    domain_count(filename)\n",
    "    # create dataframe from csv output file from daily domain count\n",
    "    df=pd.read_csv(\"student_count.csv\",sep=\",\")\n",
    "    df_combined=pd.merge(df,studentdf,how=\"left\",left_on=\"user_id\",right_on=\"USERNAME\")\n",
    "    df_combined.columns=['user_id','date_time','url','domain','USERNAME',\"acad_prog\",\"prog_yr\"]\n",
    "    df_combined=df_combined.drop('USERNAME',axis=1)\n",
    "    df_combined['acad_prog']=df_combined['acad_prog'].fillna(\"staff\")\n",
    "    df_combined['prog_yr']=df_combined['prog_yr'].fillna(\"Other\")\n",
    "    df_combined['on_or_off']=\"offcampus\"\n",
    "    newdf = newdf.append(df_combined)  \n",
    "    newdf.to_csv('combined201808_offcampus.csv',index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
